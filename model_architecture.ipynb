{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# arXiv Recommender Architecture"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose of this notebook:\n",
    "\n",
    "1. Layout a clear blueprint of project goal\n",
    "    - Describe the pipeline between user input and project output\n",
    "    - Clearly indicate which aspects of the pipeline we will tune\n",
    "2. Clearly list the next phases of the project.\n",
    "    - Deadline: Night of **June 2**. 10 days left\n",
    "    - Break these into subtasks and create a log to keep track of progress."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project goal\n",
    "\n",
    "### Main goal (Rapid prototype)\n",
    "\n",
    "User input: a list of papers they are interested in\n",
    "\n",
    "Model output: top 10 papers within a fixed library that are 'most similar' to the input set\n",
    "\n",
    "Components of this model: \n",
    "\n",
    "(Step 0) Prepare the library\n",
    "- Clean and vectorize the papers\n",
    "- Run clustering to organize the library into topics\n",
    "\n",
    "1. Process the input set\n",
    "    - Clean and vectorize\n",
    "    - obtain the 3 most likely topics each input paper belongs to\n",
    "1. For each input paper, find candidate papers to recommend.\n",
    "    - Search among the top 3 most likely topics to find its nearest neighbors\n",
    "1. Reduce the candidate recommendations to the top 10 'best' recommendations\n",
    "    - Choose some scheme for doing this\n",
    "1. Return the recommendations in a human-readable format\n",
    "\n",
    "### By 'a model' we mean a choice of how to carry out these steps\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current state of progress\n",
    "\n",
    "1. Current library is 'df_experiment'\n",
    "- ~4400 papers cleaned and pre-processed\n",
    "- Roughly 1000 per each of the subjects: diff geo, math phys, pdes, quantum algebra, rep theory\n",
    "2. Jenia and Ethan's code can vectorize this library according to different vectorization schemes\n",
    "- Bag of words/word count\n",
    "- tf-idf\n",
    "- word2vec\n",
    "3. Given an input paper, we can vectorize it using the same code and compute its nearest neighbors wrt cosine distance\n",
    "\n",
    "### What we do not yet have\n",
    "\n",
    "- Clustering into topics for any of the three vectorization schemes above\n",
    "- A function that can take in the arxiv id of an input paper and output the top X closest papers in the library\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggested next steps\n",
    "\n",
    "1. Build a bare-bones full model as above (with clustering) that can perform the goal task (maybe badly)\n",
    "1. Spend the rest of the time looking into tuning each individual aspect of the model pipeline to achieve best results\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "Building a model consists of making the following choices:\n",
    "\n",
    "1. The choice of (and pre-processing of) the library from which we will pull recommendations\n",
    "- Per Andrew's advice, expand our library size and subject breadth\n",
    "- Practical size is constrained by the speed at which we can vectorize and cluster it\n",
    "- Perhaps give some exploratory breakdown of topics by arxiv subject tag before analyzing\n",
    "2. The choice of method to vectorize the text\n",
    "- Want to use sentence transformers to get the best results\n",
    "- How to choose among all of the pre-trained models available?\n",
    "- Ways to optimize the speed at which it acts on our library?\n",
    "3. The choice of how to cluster by topic\n",
    "- BERTopic combines this with the previous step; it can vectorize the text using a transformer of our choice and then cluster it\n",
    "- We have design choices regarding how to do dimension reduction (PCA, t-SNE) and which clustering algorithm to use (HDBSCAN, K-means) \n",
    "4. The choice of the notion of distance or 'similarity' between papers in the embedding space.\n",
    "- Used to pull the nearest neighbors of a new input\n",
    "- Cosine distance is standard, what are other candidates?\n",
    "5. The choice of reduction from all candidate recommendations to the top 10 best"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default choices to be tuned later\n",
    "\n",
    "1. BERTopic's default sentence transformer\n",
    "2. UMAP and HDBSCAN default parameters will yield clustering with no further choices by us\n",
    "3. Cosine distance for finding nearest neighbors\n",
    "4. Take the top 10 closest in terms of distance to the *set* of inputs?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ways to tune this architecture\n",
    "\n",
    "See https://maartengr.github.io/BERTopic/getting_started/parameter%20tuning/parametertuning.html#n_components for list of hyperparameters within BERTopic that can be tuned.\n",
    "\n",
    "1. Sentence Transformer\n",
    "- There are a whole zoo of sentence transformers on hugging face, they can be imported and plugged directly into BERTopic.\n",
    "- What makes these different? What are they trained on? \n",
    "- Does this choice affect the performance of the recommender in a meaningful way?\n",
    "2. UMAP\n",
    "- n_neighbors - ~scale at which to approximate the topology of the high dim'l data. Perhaps we can get larger topic clusters by looking at larger scale features.\n",
    "- n_components - # of dimensions after reducing. The lower, the more info is destroyed, the higher, the harder it is\n",
    "for HDBSCAN to cluster well\n",
    "3. HDBSCAN\n",
    "- min_cluster_size - **one of the most important** by default, clusters can be as small as 10 points. Increasing this will decrease the number of clusters. \n",
    "- metric - the choice of distance used in the clustering algorithm. Something to note -- UMAP does *not* preserve absolute distances between the data. Regions of tightly packed data in high dimensions are treated the same as more spread out regions, therefore using euclidean distance to detect clustering after UMAP may not be capturing how clustered the data is in the original embedding.\n",
    "4. More nuanced ways to create the 10 best recommendations?\n",
    "- One possible idea: If clustering is effective on small sets of data (e.g. 30 papers of interest) detect small clusters of ~5 or so, and replace each by their means. To generate recs, take the closest papers to these means.\n",
    "- Another idea: the \"best\" rec for each input paper? or \n",
    "- a collection of recs based on the aggregation of their input (so something like recommendations close to the average input)? or maybe \n",
    "- the user sets a threshold for similarity score, above which any paper with a higher score than the threshold (relative to either a single input paper or the aggregate collection) is recommended? or even\n",
    "- Let the user choose. (this would require a little more coding on our end)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
