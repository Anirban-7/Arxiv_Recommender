{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "XgaTlGbd0IPq",
        "l6CIMIA55H-u",
        "Nu7htUXJj54L",
        "f-ukrzdl_YVx",
        "_zRZRWFu_Vnn",
        "3wT2JU78LeYU",
        "JHQesK-BLbT1",
        "WMhhOr4EwJ4w",
        "SEeSxB7FweB_",
        "fOFJrXmVwpzb",
        "MWbdMPCMTtMN",
        "KoJeeOUil1IR",
        "tP3bSzKLrc3D"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Heirarchal topic modeling analysis"
      ],
      "metadata": {
        "id": "fwb6boWwz1Ac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Goal\n",
        "\n",
        "\n",
        " We perform a topic analysis on a dataset consisting of arxiv pre-prints based on their titles and abstracts.\n",
        "\n",
        "## The dataset\n",
        "\n",
        "Our dataset contains the metadata from a uniform sample of 20,000 papers among those with subject tags in the following list:\n",
        "\n",
        "Dynamical systems, PDEs, Mathematical Physics, Probability, and Differential Geometry.\n",
        "\n",
        "## Layout of this notebook\n",
        "\n",
        "1. Preliminary analysis of the data\n",
        "1. Creating the basic topic model structure\n",
        "1. Creating the evaluation metrics\n",
        "1. Tuning hyper-parameters\n",
        "1. Evaluating performance of the model on a test set\n"
      ],
      "metadata": {
        "id": "45iQJ14g0oZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/Anirban-7/Arxiv_Recommender"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tS8K0qHpa1Hu",
        "outputId": "d5ef69c1-afc8-4ffd-9043-680e82902f41"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Arxiv_Recommender'...\n",
            "remote: Enumerating objects: 383, done.\u001b[K\n",
            "remote: Counting objects: 100% (119/119), done.\u001b[K\n",
            "remote: Compressing objects: 100% (76/76), done.\u001b[K\n",
            "remote: Total 383 (delta 57), reused 98 (delta 43), pack-reused 264\u001b[K\n",
            "Receiving objects: 100% (383/383), 611.09 MiB | 19.75 MiB/s, done.\n",
            "Resolving deltas: 100% (169/169), done.\n",
            "Updating files: 100% (84/84), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/Arxiv_Recommender/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xSSYh9Qa2w4",
        "outputId": "7352af27-1515-4f50-981f-3d1a06125e8a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Arxiv_Recommender\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Create the basic topic model structure"
      ],
      "metadata": {
        "id": "XgaTlGbd0IPq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the basic UMAP, KMeans, and HDBSCAN objects we will modify when tuning hyper-parameters\n",
        "\n",
        "In the sections below we create two instances of BERTopic models. One will be responsible for the initial K-means clustering and the second will be the template for the 5 topic models fit on each cluster."
      ],
      "metadata": {
        "id": "l6CIMIA55H-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Install necessary packages\n",
        "!pip install arxiv\n",
        "!pip install bertopic\n",
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3miLxAik0ZpW",
        "outputId": "d32c9abc-f084-4d99-967d-1051b2dac1a4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting arxiv\n",
            "  Downloading arxiv-1.4.7-py3-none-any.whl (12 kB)\n",
            "Collecting feedparser (from arxiv)\n",
            "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sgmllib3k (from feedparser->arxiv)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=b9409bc3838741d6b8afa32f3fa8335013f1ed75eebf7ec5e113496daa96958c\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
            "Successfully installed arxiv-1.4.7 feedparser-6.0.10 sgmllib3k-1.0.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bertopic\n",
            "  Downloading bertopic-0.15.0-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/143.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.22.4)\n",
            "Collecting hdbscan>=0.8.29 (from bertopic)\n",
            "  Downloading hdbscan-0.8.29.tar.gz (5.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting umap-learn>=0.5.0 (from bertopic)\n",
            "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.2/88.2 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.2.2)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (4.65.0)\n",
            "Collecting sentence-transformers>=0.4.1 (from bertopic)\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (5.13.1)\n",
            "Requirement already satisfied: cython>=0.27 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (0.29.34)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic) (2022.7.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=4.7.0->bertopic) (8.2.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.1.0)\n",
            "Collecting transformers<5.0.0,>=4.6.0 (from sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m94.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.15.2+cu118)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (3.8.1)\n",
            "Collecting sentencepiece (from sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0 (from sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numba>=0.49 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.0->bertopic) (0.56.4)\n",
            "Collecting pynndescent>=0.5 (from umap-learn>=0.5.0->bertopic)\n",
            "  Downloading pynndescent-0.5.10.tar.gz (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.12.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2023.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (23.1)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (67.7.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.1.5->bertopic) (1.16.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (16.0.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers>=0.4.1->bertopic) (8.1.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers>=0.4.1->bertopic) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n",
            "Building wheels for collected packages: hdbscan, sentence-transformers, umap-learn, pynndescent\n",
            "  Building wheel for hdbscan (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdbscan: filename=hdbscan-0.8.29-cp310-cp310-linux_x86_64.whl size=3541962 sha256=382207ac1ca819a5ee341014c087925fff0abdbd38f1990d7cb93300087379a3\n",
            "  Stored in directory: /root/.cache/pip/wheels/dc/52/e3/6c6b60b126b4d5c4370cb5ac071b82950f91649d62d72f7f56\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125926 sha256=76d9b08675ded4352a613c6bd3d037b7e207d1df42c9ae53783798f4426f2126\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82816 sha256=0060d4deae49f58254a947d8ed349cde5aad4e9c5848b92853c60502bf987058\n",
            "  Stored in directory: /root/.cache/pip/wheels/a0/e8/c6/a37ea663620bd5200ea1ba0907ab3c217042c1d035ef606acc\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.10-py3-none-any.whl size=55622 sha256=ecc5428fa07eec41bd6844d7359a54fc7d5274f4d14ab5fc6d59404b8e41a873\n",
            "  Stored in directory: /root/.cache/pip/wheels/4a/38/5d/f60a40a66a9512b7e5e83517ebc2d1b42d857be97d135f1096\n",
            "Successfully built hdbscan sentence-transformers umap-learn pynndescent\n",
            "Installing collected packages: tokenizers, sentencepiece, huggingface-hub, transformers, pynndescent, hdbscan, umap-learn, sentence-transformers, bertopic\n",
            "Successfully installed bertopic-0.15.0 hdbscan-0.8.29 huggingface-hub-0.15.1 pynndescent-0.5.10 sentence-transformers-2.2.2 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.29.2 umap-learn-0.5.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.29.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.15.2+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.15.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (16.0.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.13.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Imports\n",
        "from umap import UMAP\n",
        "from hdbscan import HDBSCAN\n",
        "from sklearn.cluster import KMeans\n",
        "from bertopic import BERTopic\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from bertopic.representation import MaximalMarginalRelevance\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import data_utils"
      ],
      "metadata": {
        "id": "Vmyy3sCy5aZc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the two-step model and define fitting and predicting methods."
      ],
      "metadata": {
        "id": "Nu7htUXJj54L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first decide the hyper-parameters we will tune. Note that we use the same bertopic model for each cluster model in order to simplify the procedure. Therefore we have \n",
        "\n",
        "We need to choose parameters of **two** bertopic models. We won't modify the respresentation of topics but rather the UMAP and clustering parameters.\n",
        "\n",
        "Model 1: UMAP and K-means clustering parameters.\n",
        "Model 2: UMAP and HDBSCAN clustering parameters.\n",
        "\n",
        "We write a function which takes two arguments model_1_params and model_2_params.\n",
        "it returns a tuple (kmeans_model , cluster_model). The second we will run inside every cluster produced by the first.\n",
        "\n",
        "To input the parameters of the models, we use a dictionary \n",
        "\n",
        "kmeans_model_params = { 'umap' : umap_params }\n",
        "cluster_model_params = {'umap': umap_params , 'hdbscan': hdbscan_params}\n",
        "\n",
        "Note that we don't change the kmeans clusterer itself because there are essentially no parameters to tune.\n",
        "\n",
        "Each of the umap and hdbscan parameters will be packaged as a kwarg and unpacked with **.\n",
        "\n",
        "umap_params = {'n_neighbors':15 , 'n_components':5, 'metric':'euclidean','min_dist':0.0, 'random_state':623}\n",
        "\n",
        "hdbscan_params = {'min_cluster_size':10, 'min_samples' : 10, 'max_cluster_size' : 0, 'metric' : 'euclidean'}\n"
      ],
      "metadata": {
        "id": "sqZdZHWiXDCP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the function which assembles the model parameters"
      ],
      "metadata": {
        "id": "f-ukrzdl_YVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Fix the parameters we will vary and construct the full set of model parameters from these\n",
        "\n",
        "def get_model_params(umap_n_neighbors=15, umap_n_components=5,hdbscan_min_cluster_size=10, hdbscan_min_samples=10):\n",
        "\n",
        "  umap_params = {'n_neighbors':15 , 'n_components':5, 'metric':'euclidean','min_dist':0.0, 'random_state':623}\n",
        "  hdbscan_params = {'min_cluster_size':10, 'min_samples' : 10, 'max_cluster_size' : 0, 'metric' : 'euclidean', 'prediction_data':'True'}\n",
        "\n",
        "  kmeans_model_params = {'umap' : umap_params} \n",
        "  cluster_model_params = {'umap' : umap_params, 'hdbscan': hdbscan_params}\n",
        "\n",
        "  return kmeans_model_params , cluster_model_params"
      ],
      "metadata": {
        "id": "FS5mZghTYzZf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the construct models function"
      ],
      "metadata": {
        "id": "_zRZRWFu_Vnn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def construct_models(kmeans_model_params , cluster_model_params):\n",
        "  # Construct umap objects \n",
        "\n",
        "  kmeans_proj = UMAP(**kmeans_model_params['umap'])\n",
        "  cluster_proj = UMAP(**cluster_model_params['umap'])\n",
        "\n",
        "  # Construct clusterers\n",
        "  kmeans_clusterer = KMeans(n_clusters=5)\n",
        "  hdbscan_clusterer = HDBSCAN(**cluster_model_params['hdbscan'])\n",
        "\n",
        "  # Construct topic representation\n",
        "  vectorizer = CountVectorizer(stop_words='english',ngram_range=(1,2))\n",
        "  rep_model = MaximalMarginalRelevance(diversity=0.5)\n",
        "\n",
        "  # K-means\n",
        "  base_topic_model = BERTopic(umap_model=kmeans_proj,\n",
        "                              hdbscan_model=kmeans_clusterer,\n",
        "                              vectorizer_model=vectorizer,\n",
        "                              representation_model=rep_model,\n",
        "                              verbose=True)\n",
        "\n",
        "  # Fine clustering\n",
        "  cluster_topic_model = BERTopic(umap_model=cluster_proj,\n",
        "                              hdbscan_model=hdbscan_clusterer,\n",
        "                              vectorizer_model=vectorizer,\n",
        "                              representation_model=rep_model,\n",
        "                              verbose=True) \n",
        "\n",
        "  return base_topic_model , cluster_topic_model"
      ],
      "metadata": {
        "id": "25Mi3aD6qblU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create the fit_model function"
      ],
      "metadata": {
        "id": "3wT2JU78LeYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Define the function which trains the models. More precisely, we are using the dataframe 'library.parquet' which contains the columns -- 'doc_strings' and\n",
        "## 'doc_strings_reduced'. This is the corpus of papers on which we do topic analysis. These columns are the exact text strings that are fed into the specter\n",
        "## sentence embedding model to generate the vector embeddings we cluster. The 'reduced' argument tells us whether to use the titles + abstracts which have had\n",
        "## rare words removed as well as latex (reduced = True) vs just the latex removed.\n",
        "\n",
        "def fit_models(base_topic_model,cluster_topic_model,reduced=False):\n",
        "  \"\"\"\n",
        "  Arguments:\n",
        "\n",
        "  reduced: Boolean determining whether we use the reduced title + abstract or the minimally cleaned title + abstract\n",
        "  base_topic_model: a bertopic model which does the first step k-means clustering\n",
        "  cluster_topic_model: a bertopic model which does the second step of topic identification within each cluster.\n",
        "\n",
        "  Returns:\n",
        "\n",
        "  A 3 tuple consisting of the trained kmeans model, a dictionary of trained cluster models, the dataframe returned with two additional\n",
        "  columns. The new columns are\n",
        "    1. 'kmeans_labels' : the numerical label 0-4 which corresponds to the k-means cluster the document belongs to\n",
        "    2. 'fine_topic_labels' : -1 if the document is an outlier within its cluster. Otherwise, it is a list of the keywords generated that\n",
        "    best describes the topic assigned to the document. \n",
        "  \"\"\"\n",
        "  df = pd.read_parquet('./final_data/library.parquet')\n",
        "\n",
        "  if reduced:\n",
        "    embeddings = pd.read_parquet('./final_data/library_vec_reduced_specter.parquet').values\n",
        "    docs = 'doc_string_reduced'\n",
        "  else:\n",
        "    embeddings = pd.read_parquet('./final_data/library_vec_specter.parquet').values\n",
        "    docs = 'doc_string'\n",
        "\n",
        "  # First train the K-means model.\n",
        "  print('Finding the K-means clusters...')\n",
        "  base_topic_model.fit(documents=df[docs].to_list(), embeddings=embeddings)\n",
        "\n",
        "  # Create a new column in the dataframe called 'kmeans_labels' which records the topic label for each paper\n",
        "  kmeans_labels = pd.Series(base_topic_model.topics_, index=df.index)\n",
        "  df['kmeans_labels'] = kmeans_labels\n",
        "\n",
        "  # Construct dictionary of cluster models\n",
        "  cluster_models = {i : cluster_topic_model for i in range(5)}\n",
        "\n",
        "  # Add a placeholder column for the fine topic labels\n",
        "  df['fine_topic_labels'] = 0\n",
        "\n",
        "  for i in range(5):\n",
        "    print(f'Getting topics for cluster {i}...')\n",
        "    \n",
        "    # Get the papers in kmeans topic i\n",
        "    indices = df.loc[df['kmeans_labels'] == i].index\n",
        "\n",
        "    # Get the documents in this topic\n",
        "    cluster_docs = df[docs].iloc[indices].to_list()\n",
        "\n",
        "    # Get the embeddings for these documents\n",
        "    cluster_embeddings = embeddings[indices,:]\n",
        "\n",
        "    # Train the ith model\n",
        "    cluster_models[i].fit(documents=cluster_docs,embeddings=cluster_embeddings)\n",
        "\n",
        "    # Create the topic labels dataframe\n",
        "    topics = cluster_models[i].topics_\n",
        "    labels = cluster_models[i].generate_topic_labels(nr_words=10,separator=' | ')\n",
        "\n",
        "    \n",
        "    def get_keywords(i):\n",
        "      return labels[i+1]\n",
        "\n",
        "    fine_topic_info = pd.DataFrame({'topic_number': topics}, index=indices)\n",
        "    fine_topic_info['topic_keywords'] = fine_topic_info['topic_number'].apply(func=get_keywords)\n",
        "\n",
        "    # Replace the keywords by -1 if the row is an outlier\n",
        "    fine_topic_info['topic_keywords'].loc[fine_topic_info['topic_number'] == -1] = -1\n",
        "\n",
        "    df['fine_topic_labels'].iloc[indices] = fine_topic_info['topic_keywords']\n",
        "\n",
        "  return base_topic_model , cluster_models , df\n"
      ],
      "metadata": {
        "id": "XF9rsxbm-8no"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create the predict_topics function."
      ],
      "metadata": {
        "id": "JHQesK-BLbT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## This will work very similarly to the fit function.\n",
        "\n",
        "## We assume we are given a dataframe consisting of test documents. The column 'doc_strings' contains the text that was used to generate the embedding of each document\n",
        "## (Cleaned title and abstract). The strip_cat column contains the arxiv math subject tags in the form of a list where each is represented by its two-letter code. i.e.\n",
        "## Dynamical Systems is 'DS'.\n",
        "## The goal is to return the test dataframe with the same two additional columns that the fit method constructs. \n",
        "\n",
        "def predict_topics(test_path,test_embeddings_path,trained_base_model,trained_cluster_models):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "\n",
        "\n",
        "\n",
        "  Returns: The dataframe that was passed with two additional\n",
        "  columns. The new columns are\n",
        "    1. 'kmeans_labels' : the numerical label 0-4 which corresponds to the k-means cluster the document belongs to\n",
        "    2. 'fine_topic_labels' : -1 if the document is an outlier within its cluster. Otherwise, it is a list of the keywords generated that\n",
        "    best describes the topic assigned to the document. \n",
        "  \n",
        "  \"\"\"\n",
        "\n",
        "  test = pd.read_parquet(test_path)\n",
        "  test_embeddings = pd.read_parquet(test_embeddings_path).values  \n",
        "\n",
        "\n",
        "  ## Trained cluster modes are encoded as a dictionary with keys 0-4 representing the name of K-means cluster it was trained on.\n",
        "\n",
        "  # Grab the documents the embeddings were trained on\n",
        "  docs = test['doc_string'].to_list()\n",
        "  \n",
        "  # Predict the K-means topic of each paper and store these as a series\n",
        "  print('Predicting K-means clusters for each document...')\n",
        "  kmeans_label_list , _ = trained_base_model.transform(documents=docs, embeddings=test_embeddings)\n",
        "  kmeans_labels = pd.Series(kmeans_label_list,index=test.index)\n",
        "  \n",
        "  # Add K-means labels to the dataframe\n",
        "  test['kmeans_labels'] = kmeans_labels\n",
        "\n",
        "  # Add a placeholder column for the fine topic labels\n",
        "  test['fine_topic_labels'] = 0\n",
        "\n",
        "\n",
        "  for i in range(5):\n",
        "    print(f'Predicting topic labels for cluster {i}...')\n",
        "\n",
        "    # Get the papers in kmeans topic i\n",
        "    indices = test.loc[test['kmeans_labels'] == i].index\n",
        "\n",
        "    # Get the documents in this topic\n",
        "    cluster_docs = test['doc_string'].iloc[indices].to_list()\n",
        "\n",
        "    # Get the embeddings for these documents\n",
        "    cluster_embeddings = test_embeddings[indices,:]\n",
        "\n",
        "    # Get the predicted topics for this cluster\n",
        "    topics , _ = trained_cluster_models[i].transform(documents=cluster_docs,embeddings=cluster_embeddings)\n",
        "    labels = trained_cluster_models[i].generate_topic_labels(nr_words=10,separator=' | ')\n",
        "    \n",
        "    def get_keywords(i):\n",
        "      return labels[i]\n",
        "\n",
        "    fine_topic_info = pd.DataFrame({'topic_number': topics}, index=indices)\n",
        "    fine_topic_info['topic_keywords'] = fine_topic_info['topic_number'].apply(func=get_keywords)\n",
        "\n",
        "    # Replace the keywords by -1 if the row is an outlier\n",
        "    fine_topic_info['topic_keywords'].loc[fine_topic_info['topic_number'] == -1] = -1\n",
        "\n",
        "    test['fine_topic_labels'].iloc[indices] = fine_topic_info['topic_keywords']\n",
        "\n",
        "  return test\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TmSt7MRhLo09"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Creating the evaluation metrics"
      ],
      "metadata": {
        "id": "WMhhOr4EwJ4w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What we want to measure\n",
        "\n",
        "Next we will evaluate the topic model on a dev set of 50 brand new articles that are not present in the dataset. We will measure\n",
        "\n",
        "1. The fraction of outliers per subject tag on the entire dataset\n",
        "2. The fraction of outlier predictions in the dev set\n",
        "3. The (subjective) accuracy of the predicted key-words. \n",
        "\n",
        "To the third point, the last 1/5 of the dev set consists of papers that Jee uhn and I will be confident in categorizing. The others will be a rough eye-test by non-experts."
      ],
      "metadata": {
        "id": "1U46Chh5cDYV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting outlier statistics"
      ],
      "metadata": {
        "id": "SEeSxB7FweB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def OHE_cats(df):\n",
        "    \"\"\"Return a DataFrame of one-hot-encoded categories of the library with\n",
        "    the same index as the library\n",
        "    \"\"\"\n",
        "\n",
        "    mlb = MultiLabelBinarizer()\n",
        "    OHE_array = mlb.fit_transform(df.strip_cat)\n",
        "    \n",
        "    return pd.DataFrame(OHE_array,columns=mlb.classes_,index=df.index)\n"
      ],
      "metadata": {
        "id": "Yzt-oeaDnXWf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Define a function to get outlier information. This will take in the results of predicting topics and return a dataframe\n",
        "## showing the breakdown of total # of outliers per subject tag, as well as the ratio of outliers per subject tag.\n",
        "\n",
        "def get_outlier_stats(results):\n",
        "\n",
        "  total_subject_count = OHE_cats(results).sum(axis=0)\n",
        "  outliers = results.loc[results['fine_topic_labels'] == -1] \n",
        "  outlier_subject_count = OHE_cats(outliers).sum(axis=0).fillna(value=0)\n",
        "  outlier_subject_ratio = outlier_subject_count / total_subject_count\n",
        "\n",
        "  return pd.DataFrame({'total_subject_count': total_subject_count,\n",
        "                       'outlier_subject_count' : outlier_subject_count,\n",
        "                       'outlier_subject_ratio': outlier_subject_ratio}).sort_values(by=['total_subject_count'],\n",
        "                                                                                    ascending=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "028QMKfwXXuz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting the final evaluation: Predicted topics for non-outliers plus outlier statistics per subject."
      ],
      "metadata": {
        "id": "fOFJrXmVwpzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Define a function taking in the results of topic model prediction and returning the predicted topics\n",
        "## as well as the outlier stats\n",
        "\n",
        "def eval_predictions(results):\n",
        "  \n",
        "  predicted_topics = results[['title_raw','abstract_raw','fine_topic_labels','strip_cat']].loc[results['fine_topic_labels'] != -1]\n",
        "\n",
        "  return predicted_topics , get_outlier_stats(results)"
      ],
      "metadata": {
        "id": "WyrLFnwvw6oD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Define the hyper-parameter tuning pipeline"
      ],
      "metadata": {
        "id": "MWbdMPCMTtMN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Working pipeline for parameter-tuning"
      ],
      "metadata": {
        "id": "KoJeeOUil1IR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Create a single function that builds the topic models, fits the library, and outputs the evaluation of the predictions made\n",
        "## on a given test set\n",
        "\n",
        "def eval_model(test_or_dev,\n",
        "                      reduced=False,                      \n",
        "                      umap_n_neighbors=15,\n",
        "                      umap_n_components=5,\n",
        "                      hdbscan_min_cluster_size=10,\n",
        "                      hdbscan_min_samples=10):\n",
        "  \n",
        "  print('Constructing cluster models...')\n",
        "  print()\n",
        "  \n",
        "  ## Construct model params\n",
        "  kmeans_model_params , cluster_model_params = get_model_params(umap_n_neighbors=15,\n",
        "                                                              umap_n_components=5,\n",
        "                                                              hdbscan_min_cluster_size=10,\n",
        "                                                              hdbscan_min_samples=10)\n",
        "  ## Construct the cluster models\n",
        "\n",
        "  base_topic_model , cluster_topic_model = construct_models(kmeans_model_params , cluster_model_params)\n",
        "\n",
        "  ## Fit the models\n",
        "  print('Fitting cluster models...')\n",
        "  print()\n",
        "\n",
        "  trained_base_model , trained_cluster_models , fit_library = fit_models(base_topic_model,cluster_topic_model,reduced=reduced)\n",
        "\n",
        "  ## Make predictions\n",
        "  if test_or_dev == 'dev':\n",
        "    test_path = './final_data/clean_dev_set.parquet'\n",
        "    test_embeddings_path = './final_data/dev_vec_specter.parquet'\n",
        "  elif test_or_dev == 'test':\n",
        "    test_path = './final_data/clean_test_set.parquet'\n",
        "    test_embeddings_path = './final_data/test_vec_specter.parquet'\n",
        "\n",
        "\n",
        "  test_predictions = predict_topics(test_path=test_path,\n",
        "               test_embeddings_path=test_embeddings_path,\n",
        "               trained_base_model=trained_base_model,\n",
        "               trained_cluster_models=trained_cluster_models)\n",
        "\n",
        "  ## Evaluate predictions\n",
        "  print('Getting library outlier data...')\n",
        "  print()\n",
        "\n",
        "  library_outliers = get_outlier_stats(fit_library)\n",
        "\n",
        "  print('Getting test set topic predictions & outlier data...')\n",
        "  test_topic_predictions , test_outlier_stats = eval_predictions(test_predictions)\n",
        "\n",
        "  print('Library outlier data:')\n",
        "  print()\n",
        "  print(library_outliers)\n",
        "  print()\n",
        "\n",
        "  print(f'{test_or_dev} set outlier data:')\n",
        "  print()\n",
        "  print(test_outlier_stats)\n",
        "  print()\n",
        "\n",
        "  print(f'{test_or_dev} set predictions:')\n",
        "  print()\n",
        "  print(test_topic_predictions)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GrpNJvkTlzrL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Evaluating different choices for parameters:"
      ],
      "metadata": {
        "id": "tP3bSzKLrc3D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Control: The default parameters\n",
        "\n",
        "\n",
        "--------------------\n",
        "\n",
        "UMAP:\n",
        "\n",
        "n_neighbors=10\n",
        "\n",
        "n_components = 5\n",
        "\n",
        "--------------------\n",
        "\n",
        "HDBSCAN:\n",
        "\n",
        "min_cluster_size = 10\n",
        "\n",
        "min_sample_size = 10\n"
      ],
      "metadata": {
        "id": "a9s4qxNm7pol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## First, run the default model as an example\n",
        "\n",
        "eval_model('dev')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vxAg5thIrW1s",
        "outputId": "7d4b55e6-1d09-401f-88b1-f2c9729a3793"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Constructing cluster models...\n",
            "\n",
            "Fitting cluster models...\n",
            "\n",
            "Finding the K-means clusters...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-06-02 21:18:16,434 - BERTopic - Reduced dimensionality\n",
            "2023-06-02 21:18:16,763 - BERTopic - Clustered reduced embeddings\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting topics for cluster 0...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-06-02 21:18:43,909 - BERTopic - Reduced dimensionality\n",
            "2023-06-02 21:18:44,137 - BERTopic - Clustered reduced embeddings\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting topics for cluster 1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-06-02 21:19:02,847 - BERTopic - Reduced dimensionality\n",
            "2023-06-02 21:19:03,063 - BERTopic - Clustered reduced embeddings\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting topics for cluster 2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-06-02 21:19:34,241 - BERTopic - Reduced dimensionality\n",
            "2023-06-02 21:19:34,416 - BERTopic - Clustered reduced embeddings\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting topics for cluster 3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-06-02 21:19:59,681 - BERTopic - Reduced dimensionality\n",
            "2023-06-02 21:19:59,857 - BERTopic - Clustered reduced embeddings\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting topics for cluster 4...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-06-02 21:20:14,433 - BERTopic - Reduced dimensionality\n",
            "2023-06-02 21:20:14,542 - BERTopic - Clustered reduced embeddings\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting K-means clusters for each document...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-06-02 21:20:39,121 - BERTopic - Reduced dimensionality\n",
            "2023-06-02 21:20:39,125 - BERTopic - Predicted clusters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting topic labels for cluster 0...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-06-02 21:20:45,765 - BERTopic - Reduced dimensionality\n",
            "2023-06-02 21:20:45,772 - BERTopic - Predicted clusters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting topic labels for cluster 1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-06-02 21:20:46,630 - BERTopic - Reduced dimensionality\n",
            "2023-06-02 21:20:46,638 - BERTopic - Predicted clusters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting topic labels for cluster 2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-06-02 21:20:47,516 - BERTopic - Reduced dimensionality\n",
            "2023-06-02 21:20:47,522 - BERTopic - Predicted clusters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting topic labels for cluster 3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-06-02 21:20:49,588 - BERTopic - Reduced dimensionality\n",
            "2023-06-02 21:20:49,597 - BERTopic - Predicted clusters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting topic labels for cluster 4...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-06-02 21:20:51,034 - BERTopic - Reduced dimensionality\n",
            "2023-06-02 21:20:51,041 - BERTopic - Predicted clusters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting library outlier data...\n",
            "\n",
            "Getting test set topic predictions & outlier data...\n",
            "Library outlier data:\n",
            "\n",
            "    total_subject_count  outlier_subject_count  outlier_subject_ratio\n",
            "MP                 6568                   2851               0.434074\n",
            "AP                 4961                   1988               0.400726\n",
            "PR                 4663                   1746               0.374437\n",
            "DG                 3765                   1579               0.419389\n",
            "DS                 2910                   1083               0.372165\n",
            "FA                  627                    286               0.456140\n",
            "CO                  544                    218               0.400735\n",
            "OC                  534                    180               0.337079\n",
            "CA                  518                    234               0.451737\n",
            "SP                  506                    221               0.436759\n",
            "AG                  484                    226               0.466942\n",
            "CV                  473                    230               0.486258\n",
            "NA                  390                    171               0.438462\n",
            "GT                  386                    199               0.515544\n",
            "QA                  382                    179               0.468586\n",
            "SG                  371                    153               0.412399\n",
            "ST                  291                    132               0.453608\n",
            "NT                  279                    106               0.379928\n",
            "MG                  255                    140               0.549020\n",
            "OA                  242                    113               0.466942\n",
            "RT                  236                     98               0.415254\n",
            "GR                  175                     78               0.445714\n",
            "AT                  152                     78               0.513158\n",
            "IT                  135                     56               0.414815\n",
            "RA                   89                     47               0.528090\n",
            "KT                   75                     30               0.400000\n",
            "LO                   46                     14               0.304348\n",
            "CT                   35                     16               0.457143\n",
            "GN                   32                      9               0.281250\n",
            "HO                   31                     17               0.548387\n",
            "AC                   21                     11               0.523810\n",
            "GM                   15                      8               0.533333\n",
            "\n",
            "dev set outlier data:\n",
            "\n",
            "                             total_subject_count  outlier_subject_count  \\\n",
            "Analysis of PDEs                              17                   17.0   \n",
            "Mathematical Physics                          15                   12.0   \n",
            "Differential Geometry                         14                   11.0   \n",
            "Probability                                   12                   12.0   \n",
            "Dynamical Systems                              8                    5.0   \n",
            "Algebraic Geometry                             5                    2.0   \n",
            "Classical Analysis and ODEs                    3                    3.0   \n",
            "Spectral Theory                                2                    2.0   \n",
            "Optimization and Control                       2                    2.0   \n",
            "Symplectic Geometry                            2                    2.0   \n",
            "Functional Analysis                            2                    2.0   \n",
            "Group Theory                                   1                    1.0   \n",
            "Geometric Topology                             1                    NaN   \n",
            "Algebraic Topology                             1                    1.0   \n",
            "Number Theory                                  1                    NaN   \n",
            "Numerical Analysis                             1                    1.0   \n",
            "Operator Algebras                              1                    1.0   \n",
            "Commutative Algebra                            1                    1.0   \n",
            "Quantum Algebra                                1                    1.0   \n",
            "Representation Theory                          1                    1.0   \n",
            "Rings and Algebras                             1                    1.0   \n",
            "Combinatorics                                  1                    1.0   \n",
            "Statistics Theory                              1                    1.0   \n",
            "Metric Geometry                                1                    1.0   \n",
            "\n",
            "                             outlier_subject_ratio  \n",
            "Analysis of PDEs                          1.000000  \n",
            "Mathematical Physics                      0.800000  \n",
            "Differential Geometry                     0.785714  \n",
            "Probability                               1.000000  \n",
            "Dynamical Systems                         0.625000  \n",
            "Algebraic Geometry                        0.400000  \n",
            "Classical Analysis and ODEs               1.000000  \n",
            "Spectral Theory                           1.000000  \n",
            "Optimization and Control                  1.000000  \n",
            "Symplectic Geometry                       1.000000  \n",
            "Functional Analysis                       1.000000  \n",
            "Group Theory                              1.000000  \n",
            "Geometric Topology                             NaN  \n",
            "Algebraic Topology                        1.000000  \n",
            "Number Theory                                  NaN  \n",
            "Numerical Analysis                        1.000000  \n",
            "Operator Algebras                         1.000000  \n",
            "Commutative Algebra                       1.000000  \n",
            "Quantum Algebra                           1.000000  \n",
            "Representation Theory                     1.000000  \n",
            "Rings and Algebras                        1.000000  \n",
            "Combinatorics                             1.000000  \n",
            "Statistics Theory                         1.000000  \n",
            "Metric Geometry                           1.000000  \n",
            "\n",
            "dev set predictions:\n",
            "\n",
            "                                            title_raw  \\\n",
            "2                   Clustering and Arnoux-Rauzy words   \n",
            "4   Equidistribution of iterations of holomorphic ...   \n",
            "6   Endemic Oscillations for SARS-CoV-2 Omicron --...   \n",
            "19  One-parameter discrete-time Calogero-Moser system   \n",
            "25  Higher-dimensional routes to the Standard Mode...   \n",
            "28  On the reduced space of multiplicative multive...   \n",
            "31  Sharp bounds on the height of K-semistable tor...   \n",
            "44  Enumerative geometry via the moduli space of s...   \n",
            "\n",
            "                                         abstract_raw  \\\n",
            "2   We characterize the clustering of a word under...   \n",
            "4   In this paper we analyze a certain family of h...   \n",
            "6   The SIRS model with constant vaccination and i...   \n",
            "19  We present a new type of integrable one-dimens...   \n",
            "25  In the old spirit of Kaluza-Klein, we consider...   \n",
            "28  A strict Lie $2$-algebra $\\Gamma(\\wedge^\\bulle...   \n",
            "31  Inspired by Fujita's algebro-geometric result ...   \n",
            "44  In this paper we relate volumes of moduli spac...   \n",
            "\n",
            "                                    fine_topic_labels  \\\n",
            "2   19 | complex | quantum | space | hilbert | str...   \n",
            "4   15 | string | boundary | entanglement | bulk |...   \n",
            "6   1 | lie | lie algebras | algebras | lie groups...   \n",
            "19  1 | lie | lie algebras | algebras | lie groups...   \n",
            "25  27 | hamiltonian | symplectic | flows | hamilt...   \n",
            "28  8 | gauge | theories | gauge theories | gauge ...   \n",
            "31  11 | gravity | einstein | spacetimes | conform...   \n",
            "44  11 | gravity | einstein | spacetimes | conform...   \n",
            "\n",
            "                                            strip_cat  \n",
            "2                                 [Dynamical Systems]  \n",
            "4                                 [Dynamical Systems]  \n",
            "6                                 [Dynamical Systems]  \n",
            "19                             [Mathematical Physics]  \n",
            "25      [Differential Geometry, Mathematical Physics]  \n",
            "28        [Algebraic Geometry, Differential Geometry]  \n",
            "31  [Algebraic Geometry, Differential Geometry, Nu...  \n",
            "44  [Algebraic Geometry, Geometric Topology, Mathe...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zLvqCaMK8V8Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}